import logging
import matplotlib.pyplot as plt
import numpy as np
from . import utils
import scipy.stats
import time
import pandas as pd
log = logging.getLogger(__name__)
colors = plt.get_cmap('tab10').colors


def estimate_threshold(epochs, seed=0, pst_range=None,
                       N_shuffles=500, avmode='median',
                       XC0m_threshold=0.3, save_data_resamples=False,
                       XC0m_plbounds=None, XC0m_sigbounds=None,
                       second_filter='pre-average',
                       second_filter_settings={'highpass': 300, 'lowpass': 3000, 'order': 1},
                       calc_XC0m_only=True,
                       KSs_plbounds=None, KSs_sigbounds=None,
                       XCp_near_0_plbounds=None, XCp_near_0_sigbounds=None, peak_lag_threshold=.5,
                       round_results=True, human_threshold=None):
    """
    This is code to algorithmically threshold ABR data as described in Shaheen et al 2024.
    Thresholds are generated by:
    1. Randomly splitting the trials into two groups, and calculating the median waveform for each group, followed by the normalized cross correlation between these median waveforms.
    2. This process is repeated 500 times to obtain a reshuffled cross-correlation distribution.
    3. The mean values of these distributions are computed for each level and fit with a sigmoid and a power law. The fit that provides the best mean squared error is then used, and threshold is defined as where that fit crossed a criterion (default 0.3).


Parameters
    ----------
    epochs : pd.DataFrame
        The index should contain (['polarity', 'level']).  Extra indexes will be dropped.
        Time must be in columns
    seed : int
        Seed used to initialize RandomState. Pass an integer for repeatable results.
    pst_range : list
        The pos-stimulus time range over which the cross-correlation is measured
    N_shuffles : int, default 500
        The number of times to repeat the random shuffling
    avmode : string, default 'median'
        If 'mean' create subaverages by mean
        If 'median' create them by median
    XC0m_threshold: float, default 0.3
        The criterion value of normalized cross correlation used to find threshold
    save_data_resamples: bool, default False
        If True, the results of each reshuffling are saved
    XC0m_plbounds: string, default None
        If 'increasing' sets bounds to make slope positive
    XC0m_sigbounds: string, default None
        If 'increasing, midpoint within one step of x range' sets bounds to make slope positive,
            # and midpoint within [min(level) - step, max(level) + step]. step is usually 5 dB
    second_filter: string, default None
        Applies an iir filter (forward and backward using scipy.signal.filtfilt) to each trial.
        If 'pre-avergae' applies the filter before averaging
        If 'post-avergae' applies the filter after averaging
        If None does not apply a filter.
    second_filter_settings: dict, default {'highpass': 300, 'lowpass': 3000, 'order': 1}
        The filter settings used for the second filter
    round_results: bool, default True
        If True, round results to 3 decimal places
    human_threshold : float
        The threshold as selected by a human rater. Not used in the algorithm, if passed will be indicated on the plots

    --The following parameters are used to control alternate ways of using the same cross-correlation distributions to
      find threshold. Empirically they didn't work as well.
    calc_XC0m_only : bool, default True
        Set to False to additionally calculcate (and plot) these two alternate ways.
    KSs_plbounds, KSs_sigbounds: Like above but used when using Kolmogorov-Smirnov test statistic to measure differences
        in cross correlation distribution between each level
    XCp_near_0_plbounds, XCp_near_0_sigbounds: Like above but used when using the percent of time peak cross correlation
        is near 0 for each level.
    peak_lag_threshold : float, default 0.5
        the time threshold (in ms) over which to calculate the percent of time peak cross correlation is near 0 (within +/-) this value

 Outputs
    ----------
    fit_results : dictionary of fitting results
    fig_handle : figure handle with plots of the results

    fit_results contains these keys:
        threshold: the threshold estimated by the algorithm.
            Set to -inf if all datapars['xc0mean'] are above XC0m_threshold*1.1 (multiplied by 1.1 to allow a small
              amount of extrapolation.
            Set to +inf if all datapars['xc0mean'] are below XC0m_threshold
            Set to np.nan if fitting find threshold fails (needs manual analysis)
        status:
            Success: fitting worked
            Failure: fitting failed
            Skipped: fitting skipped due to less than 100 trials for one of the stimuli
        status_message:
            If status was Success, will be one of:
              sigmoid: threshold was found with the sigmoid
              power low: threshold was found with the power law
              power law (noisy): threshold was found with the power law but was poorly fit. Review for accuracy.
            Otherwise will give details on why status is not Success
        fit_XC0m: details on the fit functions:
            sigmoid_fit: details on sigmoid fit
                params: fitting parameters
                yfit: reslt of fit
                sseL sum of squared errors
            power_law_fit: details on power law fit, parameters as for sigmoid
            bestFitType: Which function was chosed to find threshold
            algpars: criterion parameters used to find threshold
        algpars: parameters as passed in to estiamte_threshold
        datapars:
            fs: sampling frequency estimated from data
            levels: stimulus levels
            xc0mean: means of the cross correlation distributions (for 0 lag)
            xc0std:  standard deviations of the cross correlation distributions (for 0 lag)
            N_min_global: Minimum number of trials (per polarity) across all levels
        comptime: how long it took (in seconds) to run algorithm. First N elements are for each of N levels,
          last element is grand total
        plottime: how long it took (in seconds) to plot results.

       """
    t00 = time.time()
    N_subaverages = 2

    # Check that dataframe has the right indexes, drop unneeded indexes
    assert all([name in epochs.index.names for name in ['polarity', 'level']]), 'epochs dataframe must have ' \
                                                                        '"polarity" and level" as indexes'
    drop_these = [name for name in epochs.index.names if name not in ['polarity', 'level']]
    epochs.reset_index(drop_these, drop=True, inplace=True)
    epochs.index = epochs.index.reorder_levels(['polarity', 'level'])
    epochs.sort_index(inplace=True)

    # Drop empty trials
    all_values_0 = np.all(epochs == 0, axis=1)
    dropped_all = False
    if any(all_values_0):
        dropped_all = all(all_values_0)
        log.warning(f'\n {all_values_0.sum()}/{len(all_values_0)} epochs in this dataset have values of all 0.'
                    f' Dropping these epochs.')
        epochs = epochs[~all_values_0]

    #Count number of levels, polarities, trials per rep. Calculate fs (sampling frequency) for data)
    levels = epochs.index.get_level_values('level').unique().values
    polarities = epochs.index.get_level_values('polarity').unique().values
    unV, unN = np.unique(np.diff(epochs.keys().values), return_counts=True)
    fs = 1 / unV[unN.argmax()]
    # filter data if requested
    if second_filter == 'pre-average':
        epochs[:] = utils.filter(epochs.values, fs, **second_filter_settings)

    peak_lag_threshold_samples = np.ceil(peak_lag_threshold * fs / 1000)
    N_by_polarity = epochs.groupby(['polarity', 'level']).size()
    if len(N_by_polarity) == 0:
        if dropped_all:
            N_min_global = 0
        else:
            raise RuntimeError('N_by_polarity is empty but not all epochs were dropped. If this happened its a bug.')
    else:
        N_min_global = N_by_polarity.min()
    if N_min_global < 100:
        if dropped_all:
            msg = f'All epochs were dropped because they all had values of all zero.'
        else:
            msg = f'There is at least one condition with only {N_min_global} reps. Minimum needed (arbitrary) is 100.'\
              f'There are {len(epochs)} total reps for this recording'
        log.warning(msg)
        datapars = {'fs': fs, 'levels': levels}
        algpars = {'peak_lag_threshold': peak_lag_threshold,
                   'N_shuffles': N_shuffles}
        fit_results = {'threshold': np.nan, 'threshold_XCp_near_0': np.nan,
                       'fit_XCp_near_0': None, 'fit_KSs': None,
                       'peak_lag': np.nan, 'peak': np.nan, 'xc0': np.nan,
                       'xcLm': np.nan, 'xcLstd': np.nan,
                       'algpars': algpars, 'datapars': datapars}
        fit_results['threshold_XC0m'] = np.nan
        fit_results['threshold_XCp_near_0'] = np.nan
        fit_results['threshold_KSs'] = np.nan
        fit_results['fit_XCp_near_0'] = None
        fit_results['fit_KSs'] = None
        fit_results['fit_XC0m'] = None
        fit_results['comptime'] = np.nan
        fit_results['status'] = 'Skipped'
        fit_results['status_message'] = msg
        return fit_results, plt.figure()

    N_varies = len(N_by_polarity.unstack('polarity').min(axis=1).unique()) > 1
    if N_varies:
        log.warning('\n Different level combinations have different number of reps. '
                    'The number of reps per subaverage will differ by level.')
    else:
        N_per_group = int(np.floor(N_min_global / N_subaverages))

    # Calculate time window to compute correlation over
    if pst_range is None:
        N_time = len(epochs.keys())
        time_inds = np.full(N_time, True)
    else:
        time_inds = (epochs.keys().values >= pst_range[0]) & (
                epochs.keys().values < pst_range[1])
        N_time = time_inds.sum()

    # Initialize variables
    if calc_XC0m_only:
        xc0 = np.zeros((len(levels), N_shuffles))
    else:
        xc = np.zeros((len(levels), N_shuffles, N_time))
        peak_index = np.zeros((len(levels), N_shuffles), dtype=int)
        peak_lag = np.zeros((len(levels), N_shuffles), dtype=int)
        peak = np.zeros((len(levels), N_shuffles))
        lags = np.arange(N_time) - (N_time - 1) / 2
        L0 = np.where(lags == 0)[0][0]
    epochs_means = np.zeros((len(epochs.keys()), len(levels), 3))
    epochs_sems = np.zeros((len(epochs.keys()), len(levels)))
    ABRtime = epochs.keys().values.astype('float') * 1000
    rn = np.random.RandomState(seed)
    comptime = np.zeros(len(levels)+1)

    # Cross-correlate subaverages and store in a nd array for each level
    for i, level in enumerate(levels):
        t0 = time.time()
        #initialize variables for this level
        if N_varies:
            N_min = N_by_polarity.loc[:, level].min()
            N_per_group = int(np.floor(N_min / N_subaverages))
        # epochs_i = epochs.loc[:, :, level, :]
        # indP = epochs.loc[:, 1, level, :].index.get_level_values(0).values
        # indN = epochs.loc[:, -1, level, :].index.get_level_values(0).values
        epochsP = epochs.loc[(1, level), time_inds]
        epochsN = epochs.loc[(-1, level), time_inds]
        indP = np.arange(len(epochsP))
        indN = np.arange(len(epochsN))
        randomized_indicesP = np.zeros((N_per_group * N_subaverages, N_shuffles), dtype=int)
        randomized_indicesN = np.zeros((N_per_group * N_subaverages, N_shuffles), dtype=int)

        # loop across number of shuffles
        for ishuf in range(N_shuffles):
            # randomize into two buckets, evenly splitting by polarity
            randomized_indicesP[:, ishuf] = rn.choice(indP, N_per_group * N_subaverages, replace=False)
            randomized_indicesN[:, ishuf] = rn.choice(indN, N_per_group * N_subaverages, replace=False)
            # average (either mean or median)
            if avmode == 'mean':
                epochs_mean0 = epochsP.iloc[randomized_indicesP[:N_per_group, ishuf]].mean() + \
                               epochsN.iloc[randomized_indicesN[:N_per_group, ishuf]].mean()
                epochs_mean1 = epochsP.iloc[randomized_indicesP[N_per_group:, ishuf]].mean() + \
                               epochsN.iloc[randomized_indicesN[N_per_group:, ishuf]].mean()
            elif avmode == 'median':
                epochs_mean0 = np.median(np.vstack((epochsP.iloc[randomized_indicesP[:N_per_group, ishuf]].values,
                                                    epochsN.iloc[randomized_indicesN[:N_per_group, ishuf]].values)),
                                         axis=0)
                epochs_mean1 = np.median(np.vstack((epochsP.iloc[randomized_indicesP[N_per_group:, ishuf]].values,
                                                    epochsN.iloc[randomized_indicesN[N_per_group:, ishuf]].values)),
                                         axis=0)

            else:
                raise RuntimeError(f'Invalid avmode: {avmode}')
            # filter data if requested
            if second_filter == 'post-average':
                epochs_mean0 = utils.filter(epochs_mean0, fs, **second_filter_settings)
                epochs_mean1 = utils.filter(epochs_mean1, fs, **second_filter_settings)
            #calculate cross correlation (for 0 lag only if that's all that's needed)
            if calc_XC0m_only:
                xc0[i, ishuf] = np.corrcoef(epochs_mean0, epochs_mean1)[0, 1]
            else:
                xc[i, ishuf, :] = utils.crossCorr(epochs_mean0, epochs_mean1, norm=True)
        if not calc_XC0m_only:
            peak_index[i, :] = np.argmax(xc[i, :, :], axis=-1)
            peak_lag[i, :] = peak_index[i, :] - (N_time - 1) / 2
            peak[i, :] = np.max(xc[i, :, :], axis=-1)

        #calculate mean and std of waveform for this level
        epochs_means[:, i, 0] = epochs.xs(level, level='level').values.mean(axis=0)
        epochs_sems[:, i] = np.std(epochs.xs(level, level='level').values, axis=0, ddof=1) / \
                            np.sqrt(len(indP) + len(indN))
        # find which shuffle yielded a cross-corraltion value closest to the mean
        if calc_XC0m_only:
            ishuf_mean = np.argmin(np.abs(xc0[i, :] - xc0[i, :].mean()))
        else:
            ishuf_mean = np.argmin(np.abs(xc[i, :, L0] - xc[i, :, L0].mean()))
        # Create and save subaverages for this shuffle to show on plot later
        if avmode == 'mean':
            epochs_means[:, i, 1] = np.mean(
                np.vstack((epochs.loc[(1, level)].iloc[randomized_indicesP[:N_per_group, ishuf_mean]].values,
                            epochs.loc[(-1, level)].iloc[randomized_indicesN[:N_per_group, ishuf_mean]].values)),
                axis=0)
            epochs_means[:, i, 2] = np.mean(
                np.vstack((epochs.loc[(1, level)].iloc[randomized_indicesP[N_per_group:, ishuf_mean]].values,
                           epochs.loc[(-1, level)].iloc[randomized_indicesN[N_per_group:, ishuf_mean]].values)),
                axis=0)
        elif avmode == 'median':
            epochs_means[:, i, 1] = np.median(
                np.vstack((epochs.loc[(1, level)].iloc[randomized_indicesP[:N_per_group, ishuf_mean]].values,
                           epochs.loc[(-1, level)].iloc[randomized_indicesN[:N_per_group, ishuf_mean]].values)),
                axis=0)
            epochs_means[:, i, 2] = np.median(
                np.vstack((epochs.loc[(1, level)].iloc[randomized_indicesP[N_per_group:, ishuf_mean]].values,
                           epochs.loc[(-1, level)].iloc[randomized_indicesN[N_per_group:, ishuf_mean]].values)),
                axis=0)
        if second_filter == 'post-average':
            epochs_means[:, i, 0] = utils.filter(epochs_means[:, i, 0], fs, **second_filter_settings)
            epochs_means[:, i, 1] = utils.filter(epochs_means[:, i, 1], fs, **second_filter_settings)
            epochs_means[:, i, 2] = utils.filter(epochs_means[:, i, 2], fs, **second_filter_settings)
        t1 = time.time()
        comptime[i] = t1 - t0
    if not calc_XC0m_only:
        p_near_0 = (np.abs(peak_lag[:i + 1, :]) < peak_lag_threshold_samples).sum(axis=1) / N_shuffles
        xc0 = xc[:, :, L0]

    xc0mean = xc0.mean(axis=1)
    xc0std = xc0.std(axis=1)
    # fit with a sigmoid and power law, also contains logic to decide which fit to use
    fit_XC0m, threshold_XC0m = utils.fit_sigmoid_power_law(levels, xc0mean, XC0m_threshold, y_err=xc0std,
                                                           sigbounds=XC0m_sigbounds, plbounds=XC0m_plbounds)
    # If also finding threshold by alternate methods, fit those here
    if not calc_XC0m_only:
        nulld = xc[0, :, L0]
        XCpeak_pv = np.zeros(len(levels))
        XCpeak_KSs = np.zeros(len(levels))
        for i, level in enumerate(levels):
            res = scipy.stats.kstest(nulld, xc0[i, :], alternative='greater')
            XCpeak_pv[i] = res.pvalue
            XCpeak_KSs[i] = res.statistic

        thresholdCriterion = .3
        fit_KSs, threshold_KSs = utils.fit_sigmoid_power_law(levels, XCpeak_KSs, thresholdCriterion,
                                                             sigbounds=KSs_sigbounds, plbounds=KSs_plbounds)

        thresholdCriterion2 = .4
        fit_XCp_near_0, threshold_XCp_near_0 = utils.fit_sigmoid_power_law(levels, p_near_0, thresholdCriterion2,
                                                                           sigbounds=XCp_near_0_sigbounds,
                                                                           plbounds=XCp_near_0_plbounds)

    # Find levels_, a selection of levels used to plot (showing them all makes the figure too hard to read)
    threshold_level = threshold_XC0m
    if np.isinf(threshold_level):
        if threshold_level > 0:
            levels_ = levels[-12:]
        else:
            levels_ = levels[:12]
    elif np.isnan(threshold_level):
        levels_ = levels[-12:]
    else:
        try:
            threshold_level = levels[np.where(threshold_level > levels)[0][-1]]
        except:
            threshold_level = levels[np.min((len(levels)-1, 5))]
        levels_ = levels[(levels >= (threshold_level - 40)) & (levels <= (threshold_level + 30))]

    # Store results in dictionaries
    datapars = {'fs': fs, 'levels': levels, 'xc0mean': xc0mean, 'xc0std': xc0std, 'N_min_global': N_min_global}
    if save_data_resamples:
        # Only save these if requested
        datapars.update({'xc0': xc0})
        if not calc_XC0m_only:
            datapars.update({'peak_lag': peak_lag, 'peak': peak, 'xcLmean': xc.mean(axis=1),
                             'xcLstd': xc.std(axis=1)})

    if N_varies:
        datapars.update({'N_by_polarity': N_by_polarity.unstack('polarity').values.T})

    algpars = {'peak_lag_threshold': peak_lag_threshold,
               'N_shuffles': N_shuffles}
    fit_results = {'threshold': threshold_XC0m}
    if fit_XC0m['thdEstimationFailed']:
        fit_results['status'] = 'Failure'
        fit_results['status_message'] = 'curve fitting did not cross threshold'
    else:
        fit_results['status'] = 'Success'
        fit_results['status_message'] = fit_XC0m['bestFitType']

    fit_results['fit_XC0m'] = fit_XC0m
    if not calc_XC0m_only:
        fit_results['threshold_XCp_near_0'] = threshold_XCp_near_0
        fit_results['threshold_KSs'] = threshold_KSs
        fit_results['fit_XCp_near_0'] = fit_XCp_near_0
        fit_results['fit_KSs'] = fit_KSs

    fit_results['algpars'] = algpars
    fit_results['datapars'] = datapars
    t1 = time.time()
    comptime[-1] = t1 - t00
    fit_results['comptime'] = comptime

    if any(all_values_0):
        fit_results['status_message'] += f' WARNING: {all_values_0.sum()}/{len(all_values_0)} epochs had values of ' \
                                         f'all 0 and were dropped.'
    # Plot results
    t0 = time.time()
    if calc_XC0m_only:
        fig_handle = plot_fit(levels, levels_, xc0, ABRtime, epochs_means,
                       epochs_sems, pst_range, fit_XC0m, norm_waveforms=True,
                       human_threshold=human_threshold, avmode=avmode, criterion=XC0m_threshold)
    else:
        fig_handle = plot_fit_full(lags, levels, levels_, xc, xc0, peak_lag, ABRtime, epochs_means,
                            epochs_sems, pst_range, p_near_0,  XCpeak_pv,
                            XCpeak_KSs, fit_KSs, fit_XCp_near_0, fit_XC0m, norm_waveforms=True,
                            human_threshold=human_threshold, avmode=avmode)
    fit_results['plottime'] = time.time() - t0

    if round_results:
        try:
            fit_results['threshold'] = np.round(fit_results['threshold'], 2)
            fit_results['fit_XC0m']['threshold']= np.round(fit_results['fit_XC0m']['threshold'],2)
            fit_results['fit_XC0m']['sigmoid_fit']['yfit']= np.round(fit_results['fit_XC0m']['sigmoid_fit']['yfit'],3)
            fit_results['fit_XC0m']['sigmoid_fit']['sse'] = np.round(fit_results['fit_XC0m']['sigmoid_fit']['sse'],4)
            fit_results['fit_XC0m']['power_law_fit']['yfit'] = np.round(fit_results['fit_XC0m']['power_law_fit']['yfit'], 3)
            fit_results['fit_XC0m']['power_law_fit']['sse'] = np.round(fit_results['fit_XC0m']['power_law_fit']['sse'], 4)
            fit_results['fit_XC0m']['power_law_fit']['adj_r2'] = np.round(fit_results['fit_XC0m']['power_law_fit']['adj_r2'], 4)
            fit_results['datapars']['xc0mean'] = np.round(fit_results['datapars']['xc0mean'],3)
            fit_results['datapars']['xc0std'] = np.round(fit_results['datapars']['xc0std'], 3)
            fit_results['comptime'] = np.round(fit_results['comptime'], 3)
            fit_results['plottime'] = np.round(fit_results['plottime'], 3)
        except:
            pass
    return fit_results, fig_handle


def plot_fit(levels, levels_, xc0, ABRtime, epochs_means, epochs_sems, pst_range, fit_XC0m,
             norm_waveforms=True, human_threshold=None, avmode='mean', criterion=0.3):
    # In the left column the figures show mean +/- SE of all trials in black, and median (or mean, depending on AVmode)
    # for the two subsets. Waveforms are normalized (for each level all 3 lines are scaled by the peak-to-peak of
    # the mean of all trials). The right hand side shows mean correlation coefficient vs stimulus level. Sigmoid and
    # power law fits to this data are shown in green and purple. The threshold is shown by the pink dashed line.

    fs_scale = 1
    fs_labels = 10*fs_scale
    fs_ticklabels = 10*fs_scale

    udiffs, counts = np.unique(np.diff(np.array(levels)), return_counts=True)
    m = counts.argmax()
    level_diff_mode = udiffs[m]

    if pst_range is not None:
        ii = (ABRtime > pst_range[0]*1000) & (ABRtime <= pst_range[1]*1000)
    else:
        ii = np.full(ABRtime.shape, True)

    fig_handle, ax = plt.subplots(1, 2, figsize=(12, 10), gridspec_kw={'hspace': 0.07, 'wspace': .25, 'top': 1, 'bottom': 0.07,
                                                               'left': .07, 'right': 1, 'width_ratios': [.4, .6]})
    for level in levels_:
        i = np.where(level == levels)[0][0]
        if epochs_means is not None:
            y = epochs_means[:, i, 0].T * 1e6  # normalizing to full waveform

            if norm_waveforms:
                ys = 1 / (y[ii].max(axis=0) - y[ii].min(axis=0))
            else:
                ys = 1
            # plotting just windowed data
            h0 = ax[0].plot(ABRtime[ii], y[ii] * ys * level_diff_mode + level, 'k', linewidth=1)

            h0f = ax[0].fill_between(ABRtime[ii],
                        np.squeeze(epochs_means[ii, i, 0] - epochs_sems[ii, i]) * 1e6 * ys * level_diff_mode + level,
                        np.squeeze(epochs_means[ii, i, 0] + epochs_sems[ii, i]) * 1e6 * ys * level_diff_mode + level,
                        color='lightgrey', alpha=.5)

            h1 = ax[0].plot(ABRtime[ii], epochs_means[ii, i, 1] * 1e6 * ys * level_diff_mode + level, color=colors[0],
                       linewidth=1)
            h2 = ax[0].plot(ABRtime[ii], epochs_means[ii, i, 2] * 1e6 * ys * level_diff_mode + level, color=colors[1],
                       linewidth=1)
            if pst_range is not None:
                ax[0].set_xlim(np.array(pst_range) * 1000)
    if norm_waveforms:
        ax[0].set_ylim((levels_[0]-level_diff_mode*.7, levels_[-1]+ 0.7*level_diff_mode + 0.09*(levels_[-1]-levels_[0])))
    ax[0].set_xlabel('Time (ms)', fontsize=fs_labels)
    ax[0].set_ylabel('Level (dB SPL)', fontsize=fs_labels)
    ax[0].legend([(h0[0], h0f), h1[0], h2[0]], ['mean \u00B1 SE of all trials', avmode +' of 1st subset',
                                         avmode +' of 2nd subset'], loc='upper left', bbox_to_anchor=(0,1))
    ax[0].set_yticks(levels_)
    time_lines = np.array([2, 4, 6, 8])
    if pst_range is not None:
        time_lines = time_lines[(time_lines > pst_range[0]*1000) & (time_lines < pst_range[1]*1000)]
    for tl in time_lines:
        ax[0].axvline(tl, color='lightgrey', zorder=-10, linewidth=.5)

    # ax[0].set_ylabel('Level (dB SPL)')

    if fit_XC0m is not None:
        # levi = [i for i,lev in enumerate(levels) if any(lev==np.array([30,65]))]
        # h = ax[1].violinplot(xc0[levi,:].T, levels[levi], widths=np.diff(levels[:2])[0]*.9, showextrema=False, points=200,
        #                    showmeans=False)
        h = ax[1].violinplot(xc0.T, levels, widths=np.diff(levels[:2])[0]*.9, showextrema=False, points=200,
                           showmeans=False)
        for pc in h['bodies']:
            pc.set_facecolor('lightgrey')
            # pc.set_edgecolor('black')
            pc.set_alpha(.5)
        ax[1].plot(levels, xc0.mean(axis=1), '.k', label='data mean', )
        # ax[1].plot(levels[levi], xc0[levi,:].mean(axis=1), '.k', label='data mean', )
        ax[1].set_ylim((-.3, 1.01))
        yf = None
        if fit_XC0m['sigmoid_fit'] is not None:
            if fit_XC0m['bestFitType'] == 'sigmoid':
                l = f"sigmoid fit,\nthresh={fit_XC0m['threshold']:.1f}"
                lw = 2
                yf = fit_XC0m['sigmoid_fit']['yfit']
            else:
                l = f"sigmoid fit,\nnot used"
                lw = 1

            ax[1].plot(levels, fit_XC0m['sigmoid_fit']['yfit'], color=colors[2], lw=lw, label=l)
            if fit_XC0m['bestFitType'] == 'power law':
                l = f"power law fit,\nthresh={fit_XC0m['threshold']:.1f}"
                lw = 2
                ls = '-'
                yf = fit_XC0m['power_law_fit']['yfit']
            elif fit_XC0m['bestFitType'] == 'power law (noisy)':
                l = f"Noisy power law fit,\nthresh={fit_XC0m['threshold']:.1f}"
                lw = 2
                ls = '--'
                yf = fit_XC0m['power_law_fit']['yfit']
            else:
                l = f"power law fit,\nnot used"
                lw = 1
                ls = '-'
            ax[1].plot(levels, fit_XC0m['power_law_fit']['yfit'], color=colors[4], lw=lw, label=l, ls=ls)

        if fit_XC0m['threshold'] is not None:
            if fit_XC0m['threshold'] is np.inf:
                l = 'thresh=inf\n(all levels below criterion)'
            elif np.isnan(fit_XC0m['threshold']):
                l = 'thresh is nan\n(error fitting?)'
            else:
                l = None
            ax[1].axvline(fit_XC0m['threshold'], color=colors[6], linestyle='--', linewidth=1, label=l)
            time_range = np.array((ABRtime[ii][0], ABRtime[ii][-1]))
            ax[0].plot((time_range[0]-np.diff(time_range)*.25, time_range[1]+np.diff(time_range)*.05),
                       fit_XC0m['threshold']*np.ones(2), color=colors[6], linestyle='--', linewidth=1)
            ax[0].set_clip_on(False)
        ax[1].axhline(criterion, color='k', linestyle='--', linewidth=.5)
        ax[1].text(ax[1].get_xlim()[1], criterion, 'criterion', horizontalalignment='right', fontsize=10*fs_scale)
        ax[1].set_ylabel('Correlation Coefficient', fontsize=fs_labels)
        ax[1].set_yticks((0, .2, .4, .6, .8, 1))
        ax[1].set_yticklabels(('0', '.2', '.4', '.6', '.8', '1'))
        ax[1].set_xlabel('Level (dB SPL)', fontsize=fs_labels)
        levi = (np.abs(levels-np.percentile(ax[1].get_xlim(),35))).argmin()
        if (yf is None) or yf[levi] < .8:
            ax[1].legend(loc='upper left', bbox_to_anchor=(0, 1), frameon=True, fontsize=10 * fs_scale, framealpha=1)
        else:
            ax[1].legend(loc='lower right', bbox_to_anchor=(1, 0), frameon=True, fontsize=10 * fs_scale, framealpha=1)
        ax[0].tick_params(labelsize=fs_ticklabels)
        ax[1].tick_params(labelsize=fs_ticklabels)
        ax[1].set_clip_on(False)
    # Hide the right and top spines
    [ax_.spines.right.set_visible(False) for ax_ in ax]
    [ax_.spines.top.set_visible(False) for ax_ in ax]

    if human_threshold is not None:
        ax[1].axvline(human_threshold, color='k', linestyle='--', linewidth=1)
    return fig_handle


def plot_fit_full(lags, levels, levels_, xc, xc0, peak_lag, ABRtime, epochs_means, epochs_sems, pst_range, p_near_0,
                  XCpeak_pv, XCpeak_KSs, fit_KSs, fit_XCp_near_0, fit_XC0m=None, norm_waveforms=True,
                  human_threshold=None, avmode='mean', presentation=False):
    # Full plot of results. Also plots two other ways to measure threshold from the correlation distributions that
    # empirically didn't work as well. Not as polished as plot_fit

    fig_handle = plt.figure(constrained_layout=True, figsize=(22, 14)) # np.array((22,10))/1.6) # (22,14)
    subfigs = fig_handle.subfigures(1, 2, wspace=0.07, width_ratios=[3, 1.]) #wspace=0.07
    ax = subfigs[0].subplots(len(levels_), 4, sharex='col', sharey='col', gridspec_kw={'hspace':0, 'top':0.99, 'bottom':0.03})
    ax = np.flipud(ax)
    for axi,level in enumerate(levels_):
        i = np.where(level == levels)[0][0]
        if epochs_means is not None:
            y = epochs_means[:, i, 0].T * 1e6
            if norm_waveforms:
                ys = 1/np.max(np.abs(y))
            else:
                ys = 1
            ax[axi, 0].plot(ABRtime, y*ys, 'k')

            ax[axi, 0].fill_between(ABRtime,
                                  np.squeeze(epochs_means[:, i, 0] - epochs_sems[:, i]) * 1e6*ys,
                                  np.squeeze(epochs_means[:, i, 0] + epochs_sems[:, i]) * 1e6*ys, color='grey')
            ax[axi, 0].plot(ABRtime, epochs_means[:, i, 1] * 1e6*ys, color='tab:blue')
            ax[axi, 0].plot(ABRtime, epochs_means[:, i, 2] * 1e6*ys, color='tab:orange')
            # if pst_range is not None:
            #     ax[axi,0].set_xlim(np.array(pst_range)*1000)

        if xc is not None:
            mean = xc[i, :, :].mean(axis=0)
            std = xc[i, :, :].std(axis=0)
            ax[axi, 1].fill_between(lags, mean - std, mean + std, color='k', alpha=.5, zorder=1e6)
            # ax[axi, 1].plot(lags, xc[i, :, :].T, linewidth=.5)
            ax[axi, 1].plot(lags, mean,'k',linewidth=3)

        ax[axi, 2].hist(peak_lag[i, :], np.linspace(lags[0], lags[-1], 40))

        ax[axi, 3].hist(xc0[i,:], np.linspace(-.5, 1, 60))
        # ax[axi, 3].set_ylabel(f'{level:.0f} dB', rotation='horizontal')
        # ax[axi, 3].yaxis.set_label_coords(1.1, .5)
        ax[axi, 0].set_ylabel(f'{level:.0f} dB', rotation='horizontal', ha='right')

    axi=0
    ax[axi, 1].set_xlabel('lag (ms)')
    ax[axi, 1].set_ylabel('XC (norm)')
    if norm_waveforms:
        ax[axi, 0].set_ylabel('Amplitude (norm)', rotation='vertical', ha='center')
        ax[axi, 0].set_ylim([-1.2,1.2])
    else:
        ax[axi, 0].set_ylabel('Amplitude (\u03bcV)')
    ax[axi, 0].set_xlabel('Time (ms)')
    ax[axi, 2].set_xlabel('Peak lag (ms)')
    ax[axi, 3].set_xlabel('XC at 0 lag')
    ax[axi, 1].set_ylim((-.5,.95))
    # Hide the right and top spines
    [ax_.spines.right.set_visible(False) for ax_ in ax[:, 0]]
    [ax_.spines.top.set_visible(False) for ax_ in ax[:,0]]
    [ax_.spines.bottom.set_visible(False) for ax_ in ax[1:, 0]]
    [ax_.set_yticklabels('') for ax_ in ax[:, 0]]
    # yl = ax[axi,0].get_ylim()
    # [ax_.set_ylim(yl) for ax_ in ax[:,0]];

    N = 3
    plot_XCpeak_pv=False
    if fit_XC0m is not None:
        N = N + 1
    if not plot_XCpeak_pv:
        N = N - 1
    ax2 = subfigs[1].subplots(N, 1)
    if plot_XCpeak_pv:
        ax2[0].plot(levels, XCpeak_pv, '.-')
        ii = [0, 1, 2, 3]
    else:
        ax2 = np.concatenate(([0],ax2))
        ii = [1, 2, 3]
    ax2[1].plot(levels, XCpeak_KSs, '.-', label='data')
    ax2[-1].plot(levels, p_near_0*100, '.-')
    if fit_KSs is not None:
        if type(fit_KSs) is dict:
            if fit_KSs['sigmoid_fit'] is not None:
                ax2[1].plot(levels, fit_KSs['sigmoid_fit']['yfit'], color=colors[1], label='sigmoid fit')
                ax2[1].plot(levels, fit_KSs['power_law_fit']['yfit'], color=colors[2], label='power-law fit')
            if fit_KSs['threshold'] is not None:
                ax2[1].text(ax2[1].get_xlim()[1], ax2[1].get_ylim()[0],
                            f"{(fit_KSs['bestFitType'] or 'No fit')}, thresh={fit_KSs['threshold']:.1f}",
                verticalalignment='bottom', horizontalalignment='right')
                ax2[1].axvline(fit_KSs['threshold'], color='r', linestyle='--', linewidth=.5)
        else:
            ax2[1].plot(levels, fit_KSs.yfit, color=colors[1], label='sigmoid fit')
        ax2[1].legend(loc='upper right', bbox_to_anchor=(1,.9))
    if fit_XCp_near_0 is not None:
        if type(fit_KSs) is dict:
            if fit_XCp_near_0['sigmoid_fit'] is not None:
                ax2[-1].plot(levels, fit_XCp_near_0['sigmoid_fit']['yfit']*100, color=colors[1], label='sigmoid fit')
                ax2[-1].plot(levels, fit_XCp_near_0['power_law_fit']['yfit']*100, color=colors[2], label='power-law fit')
            if fit_XCp_near_0['threshold'] is not None:
                ax2[-1].text(ax2[-1].get_xlim()[1], ax2[-1].get_ylim()[0],
                        f"{(fit_XCp_near_0['bestFitType'] or 'No fit')}, thresh={fit_XCp_near_0['threshold']:.1f}",
                verticalalignment='bottom', horizontalalignment='right')
                ax2[-1].axvline(fit_XCp_near_0['threshold'], color='r', linestyle='--', linewidth=.5)
        else:
            ax2[-1].plot(levels, fit_XCp_near_0.yfit*100, color=colors[1], label='sigmoid fit')
    ax2[1].axhline(.3, color='k', linestyle='--', linewidth=.5)
    ax2[1].text(ax2[1].get_xlim()[1], .31, 'criterion', horizontalalignment='right')
    ax2[-1].axhline(.4*100, color='k', linestyle='--', linewidth=.5)
    ax2[-1].text(ax2[-1].get_xlim()[1], 41, 'criterion', horizontalalignment='right')
    if plot_XCpeak_pv:
        ax2[0].set_ylabel('XC0_pval')
    ax2[1].set_ylabel('XC0_KSs')
    ax2[-1].set_ylabel('XCp_near_0')
    ax2[-1].set_xlabel('Level (dB SPL)')

    if fit_XC0m is not None:
        ax2[2].errorbar(levels, xc0.mean(axis=1), xc0.std(axis=1))
        ax2[2].set_ylim((-.3, 1.1))
        if fit_XC0m['sigmoid_fit'] is not None:
            ax2[2].plot(levels, fit_XC0m['sigmoid_fit']['yfit'], color=colors[1], label='sigmoid fit')
            ax2[2].plot(levels, fit_XC0m['power_law_fit']['yfit'], color=colors[2], label='power-law fit')
        if fit_XC0m['threshold'] is not None:
            ax2[2].text(ax2[2].get_xlim()[1], ax2[2].get_ylim()[0],
                f"{(fit_XC0m['bestFitType'] or 'No fit')}, thresh={fit_XC0m['threshold']:.1f}",
                verticalalignment='bottom', horizontalalignment='right')
            ax2[2].axvline(fit_XC0m['threshold'], color='r', linestyle='--', linewidth=.5)
        ax2[2].axhline(.3, color='k', linestyle='--', linewidth=.5)
        ax2[2].text(ax2[2].get_xlim()[1], .31, 'criterion', horizontalalignment='right')
        ax2[2].set_ylabel('XC0_mean')
        ax2[2].set_yticks((0,.2,.4,.6,.8,1))
        ax2[-1].set_xlabel('Level (dB SPL)')

    if human_threshold is not None:
        [ax_.axvline(human_threshold, color='k', linestyle='--', linewidth=.5) for ax_ in ax2[ii]]
    ax2[1].set_ylabel('KStest on XC at 0 lag')
    ax2[2].set_ylabel('Mean of XC at 0 lag')
    ax2[3].set_ylabel('% XC peaks near 0 lag')
    return fig_handle


